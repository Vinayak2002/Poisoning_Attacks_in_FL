Federated Learning was introduced by Google where they used this approach to improve their GBoard suggestions.
Federated Learning, also known as Collaborative Learning, enables the training of a machine learning model over a distributed network of devices with the datasets specific to the devices. This approach ensures that the server has no access to the user’s private data which is used for the training of the local model. Also, the server has no control over the client's actions. The server can only initiate rounds and select participating clients after that all actions done by clients are independent of the server’s control. This introduces vulnerabilities such as Abnormal Client Behaviour [4] which have been researched before and their solutions are introduced using a pre-trained anomaly detection algorithm. Other than this there are many attacks from which Federated Learning suffers. 

Poisoning and Inference Attacks are two such attacks that heavily impact the performance and privacy of the Federated Learning approach. Poisoning Attacks are initiated by a malicious participant who has intentions to corrupt the central model which is present on the server by introducing incorrect local model parameters to the central server. Work has been done to reduce the impact of this attack [5]. However, the approach discussed in [5] fails in privacy-preserving federated learning schemes which is a major drawback since privacy can not be compromised in most cases. Inference Attacks are attacks where an attacker may perform Model Inversion [6] and extract sensitive data from the local model parameters. 

In this report, we discuss the effect and prevention of poisoning attacks on the FL model. We have simulated the FL model using the python programming language and TensorFlow library. The performance metric used is accuracy in order to determine the effect of poisoning and the effectiveness of the protection mechanism.
