# Poisoning Attacks on Federated Learning Model
<hr />

# Technologies used:

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54) <br />
![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white) <br />
![Anaconda](https://img.shields.io/badge/Anaconda-%2344A833.svg?style=for-the-badge&logo=anaconda&logoColor=white) <br />

# What is Federated Learning:
Federated Learning was introduced by Google where they used this approach to improve their GBoard suggestions.
FL enables the training of a machine learning model over a distributed network of devices with the datasets specific to the devices. This approach ensures that the server has no access to the user’s private data which is used for the training of the local model. <hr />

# Attacks on Federated Learning:
Poisoning and Inference Attacks are two such attacks that heavily impact the performance and privacy of the Federated Learning approach. Poisoning Attacks are initiated by a malicious participant who has intentions to corrupt the central model which is present on the server by introducing incorrect local model parameters to the central server. <hr />

# What have we done?
We have simulated FL for Digit Recognintion Model using python language and recorded the impact on results.

Made with ❤️ at IIIT NAYA RAIPUR

