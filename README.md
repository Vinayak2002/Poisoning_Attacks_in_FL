Federated Learning was introduced by Google where they used this approach to improve their GBoard suggestions.
Federated Learning, also known as Collaborative Learning, enables the training of a machine learning model over a distributed network of devices with the datasets specific to the devices. This approach ensures that the server has no access to the user’s private data which is used for the training of the local model. Also, the server has no control over the client's actions. The server can only initiate rounds and select participating clients after that all actions done by clients are independent of the server’s control. This introduces vulnerabilities such as Abnormal Client Behaviour [4] which have been researched before and their solutions are introduced using a pre-trained anomaly detection algorithm. Other than this there are many attacks from which Federated Learning suffers. 

Poisoning and Inference Attacks are two such attacks that heavily impact the performance and privacy of the Federated Learning approach. Poisoning Attacks are initiated by a malicious participant who has intentions to corrupt the central model which is present on the server by introducing incorrect local model parameters to the central server. Work has been done to reduce the impact of this attack [5]. However, the approach discussed in [5] fails in privacy-preserving federated learning schemes which is a major drawback since privacy can not be compromised in most cases. Inference Attacks are attacks where an attacker may perform Model Inversion [6] and extract sensitive data from the local model parameters. 

In this report, we discuss the effect and prevention of poisoning attacks on the FL model. We have simulated the FL model using the python programming language and TensorFlow library. The performance metric used is accuracy in order to determine the effect of poisoning and the effectiveness of the protection mechanism.

There are two major types of Poisoning Attacks:
Data Poisoning: In this, the attacker targets the vulnerability of the training dataset on the client’s end. The attacker manipulates the data used for training. Label-flipping, label-modification, etc are done by attackers to mid-train the ML model and generate poisonous parameters of local training. Many approaches have been proposed to defend against data poisoning attacks in ML algorithms. Data poisoning in FL is defined as generating dirty samples to train the global model in hopes of producing falsified model parameters and sending them to the central server.

Model Poisoning: In data poisoning, the attacker manipulates the global model using poisoned data for training. In model poisoning, the attacker targets the global model accuracy by manipulating the local model parameters directly. Model poisoning attacks are usually more effective compared to data poisoning attacks. The damage caused by model poisoning attacks increases when there is a large-scale Federated Learning system where a large number of clients are required. In general, in the model poisoning attack, the malicious party can modify the updated model before sending it to the central server for central server aggregation which in turn poisons the global mode.

